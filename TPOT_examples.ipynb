{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the talk slides here -> https://slides.com/j-diegohueltesvega/data-science-lazy-people/\n",
    "\n",
    "In this nb you have some examples and ideas about how to use tpot.\n",
    "For installing tpot you can follow this guide -> http://rhiever.github.io/tpot/installing/.\n",
    "I recommend you also to install xgboost which is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1, basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),\n",
    "    iris.target.astype(np.float64), train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,n_jobs=-1)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2, titanic predictions with zero data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example doesn't have any data cleaning except the labels encoding in order to show how tpot works without any help.\n",
    "The purpose of the below function is to perform that basic \"data cleaning\", in the train set and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_labelize_titanic(filename, encoders=None):\n",
    "    \"\"\"Read csv and perform basic labeling encoding\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    if not encoders:\n",
    "        encoders = {'Sex': LabelEncoder(), \n",
    "                    'Cabin': LabelEncoder(), \n",
    "                    'Embarked': LabelEncoder()}\n",
    "        for column, encoder in encoders.items():\n",
    "            encoder.fit(list(df[column].astype(str)) + ['UnknownLabel'])\n",
    "            df[column] = encoder.transform(df[column].astype(str))\n",
    "    else:\n",
    "        for column, encoder in encoders.items():\n",
    "            df.loc[~df[column].isin(encoder.classes_), column] = 'UnknownLabel'\n",
    "            df[column] = encoder.transform(df[column].astype(str))\n",
    "        \n",
    "    df = df.fillna(-999)\n",
    "    passenger_ids = df['PassengerId']\n",
    "    df = df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n",
    "    return df, encoders, passenger_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, encoders, _ = load_and_labelize_titanic('titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you to play with the number of generations and the population size. That will impact in the optimization time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,n_jobs=-1, \n",
    "                      scoring='accuracy', cv=10)\n",
    "tpot.fit(train.drop('Survived', axis=1), train['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the same function with the test set, providing the encoders in order to transform the data in the same way. The function also returns the list of passenger ids to be used with the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test, _, passenger_ids = load_and_labelize_titanic('titanic/test.csv', encoders)\n",
    "results = tpot.predict(test)\n",
    "results_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': results})\n",
    "results_df.to_csv('titanic/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is part of the exporting from an optimized pipeline. \n",
    "You don't need to export in order to predict because you can use the tpot optimizer instance after the fit, but you can also export if you want to persist it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.8500000000000001, min_samples_leaf=4, min_samples_split=13, n_estimators=100))]), FunctionTransformer(copy)),\n",
    "    XGBClassifier(learning_rate=0.5, max_depth=6, min_child_weight=20, nthread=1, subsample=0.9000000000000001)\n",
    ")\n",
    "exported_pipeline.fit(train.drop('Survived', axis=1), train['Survived'])\n",
    "results = exported_pipeline.predict(test)\n",
    "\n",
    "results_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': results})\n",
    "results_df.to_csv('titanic/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3, Titanic predictions without data cleaning and with custom config dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that example, I want the optimizer to choose between the RF Classifier or the XGB Classifier, I also fixed these classifier parameters. Doing that, is expected that the optimizer is going to try to mutate more preprocessors and feature selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=20, population_size=100, verbosity=2, n_jobs=-1,\n",
    "                      scoring='accuracy', cv=4, config_dict={\n",
    "        'sklearn.ensemble.RandomForestClassifier': {\n",
    "            'n_estimators': [100],\n",
    "            'criterion': [\"entropy\"],\n",
    "            'max_features': [0.85],\n",
    "            'min_samples_split': [13],\n",
    "            'min_samples_leaf': [4],\n",
    "            'bootstrap': [True]\n",
    "        },\n",
    "\n",
    "        'xgboost.XGBClassifier': {\n",
    "            'n_estimators': [100],\n",
    "            'max_depth': [6],\n",
    "            'learning_rate': [0.5],\n",
    "            'subsample': [0.9],\n",
    "            'min_child_weight': [20],\n",
    "            'nthread': [1]\n",
    "        },\n",
    "\n",
    "        # Preprocesssors\n",
    "        'sklearn.preprocessing.Binarizer': {\n",
    "            'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "        },\n",
    "\n",
    "        'sklearn.decomposition.FastICA': {\n",
    "            'tol': np.arange(0.0, 1.01, 0.05)\n",
    "        },\n",
    "\n",
    "        'sklearn.cluster.FeatureAgglomeration': {\n",
    "            'linkage': ['ward', 'complete', 'average'],\n",
    "            'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.MaxAbsScaler': {\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.MinMaxScaler': {\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.Normalizer': {\n",
    "            'norm': ['l1', 'l2', 'max']\n",
    "        },\n",
    "\n",
    "        'sklearn.decomposition.PCA': {\n",
    "            'svd_solver': ['randomized'],\n",
    "            'iterated_power': range(1, 11)\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.PolynomialFeatures': {\n",
    "            'degree': [2],\n",
    "            'include_bias': [False],\n",
    "            'interaction_only': [False]\n",
    "        },\n",
    "\n",
    "        'sklearn.kernel_approximation.RBFSampler': {\n",
    "            'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.RobustScaler': {\n",
    "        },\n",
    "\n",
    "        'sklearn.preprocessing.StandardScaler': {\n",
    "        },\n",
    "\n",
    "        'tpot.built_in_operators.ZeroCount': {\n",
    "        },\n",
    "\n",
    "        # Selectors\n",
    "        'sklearn.feature_selection.SelectFwe': {\n",
    "            'alpha': np.arange(0, 0.05, 0.001),\n",
    "            'score_func': {\n",
    "                'sklearn.feature_selection.f_classif': None\n",
    "            }  # read from dependencies ! need add an exception in preprocess_args\n",
    "\n",
    "        },\n",
    "\n",
    "        'sklearn.feature_selection.SelectKBest': {\n",
    "            'k': range(1, 100),  # need check range!\n",
    "            'score_func': {\n",
    "                'sklearn.feature_selection.f_classif': None\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'sklearn.feature_selection.SelectPercentile': {\n",
    "            'percentile': range(1, 100),\n",
    "            'score_func': {\n",
    "                'sklearn.feature_selection.f_classif': None\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'sklearn.feature_selection.VarianceThreshold': {\n",
    "            'threshold': np.arange(0.05, 1.01, 0.05)\n",
    "        },\n",
    "\n",
    "        'sklearn.feature_selection.RFE': {\n",
    "            'step': np.arange(0.05, 1.01, 0.05),\n",
    "            'estimator': {\n",
    "                'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                    'n_estimators': [100],\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                    'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'sklearn.feature_selection.SelectFromModel': {\n",
    "            'threshold': np.arange(0, 1.01, 0.05),\n",
    "            'estimator': {\n",
    "                'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                    'n_estimators': [100],\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                    'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    }\n",
    "                      )\n",
    "tpot.fit(train.drop('Survived', axis=1), train['Survived'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test, _, passenger_ids = load_and_labelize_titanic('titanic/test.csv', encoders)\n",
    "results = tpot.predict(test)\n",
    "pd.DataFrame({'PassengerId': passenger_ids, 'Survived': results}).to_csv('titanic/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4, house prices regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our custom error function. Is important that the \"error\" word is in the name function. In this way, TPOT knows that should minimize the value of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmserror_log(predictions, targets):\n",
    "    return np.sqrt(((np.log(predictions) - np.log(targets)) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the last example, we're going to do zero data cleaning. We're just labeling the string columns, this time even we do it in a blind way, just iterating the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_clean_houses(filename, encoders=None):\n",
    "    df = pd.read_csv(filename)\n",
    "    if not encoders:\n",
    "        encoders ={column: LabelEncoder() \n",
    "                   for column, column_type in df.dtypes.items() \n",
    "                   if str(column_type) == 'object'}\n",
    "        for column, encoder in encoders.items():\n",
    "            encoder.fit(list(df[column].astype(str)) + ['UnknownLabel'])\n",
    "            df[column] = encoder.transform(df[column].astype(str))\n",
    "    else:\n",
    "        for column, encoder in encoders.items():\n",
    "            df.loc[~df[column].isin(encoder.classes_), column] = 'UnknownLabel'\n",
    "            df[column] = encoder.transform(df[column].astype(str))\n",
    "    \n",
    "    df = df.fillna(-999)\n",
    "    ids = df['Id']\n",
    "    df = df.drop(['Id'], axis=1)\n",
    "    return df, encoders, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, encoders, _ = load_and_clean_houses('houses/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, \n",
    "                     n_jobs=-1, scoring=rmserror_log)\n",
    "tpot.fit(train.drop('SalePrice', axis=1), train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test, _, ids = load_and_clean_houses('houses/test.csv', encoders)\n",
    "\n",
    "results = tpot.predict(test)\n",
    "result_df = pd.DataFrame({'PassengerId': ids, 'Survived': results})\n",
    "result_df.to_csv('houses/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of a exported pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", RidgeCV())]), FunctionTransformer(copy)),\n",
    "    make_union(VotingClassifier([(\"est\", ElasticNetCV(l1_ratio=0.4, tol=0.0001))]), FunctionTransformer(copy)),\n",
    "    XGBRegressor(max_depth=4, min_child_weight=1, nthread=1, subsample=0.9000000000000001)\n",
    ")\n",
    "exported_pipeline.fit(train.drop('SalePrice', axis=1), train['SalePrice'])\n",
    "results = exported_pipeline.predict(test)\n",
    "\n",
    "result_df = pd.DataFrame({'PassengerId': ids, 'Survived': results})\n",
    "result_df.to_csv('houses/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
